{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"},{"sourceId":9203638,"sourceType":"datasetVersion","datasetId":4522892}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install additional dependencies","metadata":{"_uuid":"44749934-0e25-437c-871c-4ce75663f31e","_cell_guid":"2abd5426-0af2-4cdb-ba3d-9d148c41e567","trusted":true}},{"cell_type":"code","source":"!pip install torchtyping","metadata":{"_uuid":"8e598397-0557-439f-a078-63a4b2d77e20","_cell_guid":"49673c87-7f1d-4ff8-88ee-1734d99d6668","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:46:49.823559Z","iopub.execute_input":"2024-09-30T18:46:49.824359Z","iopub.status.idle":"2024-09-30T18:47:04.456003Z","shell.execute_reply.started":"2024-09-30T18:46:49.824317Z","shell.execute_reply":"2024-09-30T18:47:04.454943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"55dcd9d4-7a09-4fe8-a809-dea59bf3180c","_cell_guid":"0e54622c-3dc2-4746-86e3-db211ffe066b","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"1653acba-b489-4cf5-ad43-6caf432f666e","_cell_guid":"1589177f-4b15-487b-91a0-cb9c04e44d97","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:04.458275Z","iopub.execute_input":"2024-09-30T18:47:04.458965Z","iopub.status.idle":"2024-09-30T18:47:04.827854Z","shell.execute_reply.started":"2024-09-30T18:47:04.458917Z","shell.execute_reply":"2024-09-30T18:47:04.827049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchtyping import TensorType\n\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import v2\n\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group","metadata":{"_uuid":"e36702ab-d96a-4ff1-ab58-fcf271c26dc9","_cell_guid":"ed0d9026-0281-45bf-a5b3-d87d79610330","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:04.829030Z","iopub.execute_input":"2024-09-30T18:47:04.829769Z","iopub.status.idle":"2024-09-30T18:47:09.977920Z","shell.execute_reply.started":"2024-09-30T18:47:04.829723Z","shell.execute_reply":"2024-09-30T18:47:09.977114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import date","metadata":{"_uuid":"c5ed2218-a110-458a-bf3c-87c5bc9fa26d","_cell_guid":"58e0e0b1-1d97-4652-bcae-44f05d800d95","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:09.979934Z","iopub.execute_input":"2024-09-30T18:47:09.980351Z","iopub.status.idle":"2024-09-30T18:47:09.984303Z","shell.execute_reply.started":"2024-09-30T18:47:09.980316Z","shell.execute_reply":"2024-09-30T18:47:09.983317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ddp_setup(rank: int, world_size: int):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    torch.cuda.set_device(rank)\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)","metadata":{"_uuid":"af741c7a-649e-44ce-8c67-77174f9fe9d5","_cell_guid":"f5681705-bcc0-4513-a477-66cf674bea78","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:09.986221Z","iopub.execute_input":"2024-09-30T18:47:09.986534Z","iopub.status.idle":"2024-09-30T18:47:10.000329Z","shell.execute_reply.started":"2024-09-30T18:47:09.986503Z","shell.execute_reply":"2024-09-30T18:47:09.999530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vision Transformer Model","metadata":{"_uuid":"1b9abcda-cc7b-43b6-8fc8-db31346baa41","_cell_guid":"8b9023e0-8442-4107-8688-ea0e54ce3561","trusted":true}},{"cell_type":"code","source":"class PatchAndPositionEmbedding(nn.Module):\n    \"\"\"\n    Combines patch and position embeddings for Vision Transformer (ViT) models.\n\n    Args:\n        image_size (int, optional): Input image size (square). Default is 384.\n        patch_size (int, optional): Patch size. Default is 16.\n        embed_dim (int, optional): Embedding dimension. Default is 192.\n        n_channels (int, optional): Number of input channels (e.g., 3 for RGB). Default is 3.\n\n    Attributes:\n        n_patches (int): Number of patches in the image.\n        linear_projection (nn.Linear): Projects patches to embedding dimension.\n        cls_token (nn.Parameter): Learnable class token.\n        pos_embedding (nn.Parameter): Learnable position embeddings.\n        unfold (nn.Unfold): Extracts image patches.\n    \"\"\"\n    \n    def __init__(self, image_size: int = 224, patch_size: int = 16, embed_dim: int = 192, n_channels: int = 3) -> None:\n        \"\"\"\n        Initializes the PatchAndPositionEmbedding module.\n\n        Args:\n            image_size (int, optional): Size of the input image (assumed square). Default is 384.\n            patch_size (int, optional): Size of each patch to be extracted from the image. Default is 16.\n            embed_dim (int, optional): Dimensionality of the output embeddings. Default is 192.\n            n_channels (int, optional): Number of input channels (e.g., 3 for RGB). Default is 3.\n        \"\"\"\n        \n        super(PatchAndPositionEmbedding, self).__init__()\n        \n        assert image_size % patch_size == 0, f\"image_size of {image_size} is not divisible by patch_size of {patch_size}\"\n        \n        self.n_patches = image_size * image_size // patch_size ** 2\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.n_channels = n_channels\n        \n        self.linear_projection = nn.Linear(self.patch_size ** 2 * self.n_channels, self.embed_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1, self.embed_dim))\n        self.pos_embedding = nn.Parameter(torch.zeros(self.n_patches + 1, self.embed_dim))\n        self.unfold = nn.Unfold(kernel_size=self.patch_size, stride=self.patch_size)\n        \n        \n    def forward(self, x: TensorType[torch.float32]) -> TensorType[torch.float32]:\n        \"\"\"\n        Forward pass that computes patch embeddings, prepends the class token, and adds positional embeddings.\n\n        Args:\n            x (TensorType[torch.float32]): Input tensor of shape (C, H, W), where C is the number of channels, \n                              and H, W are the height and width of the image.\n\n        Returns:\n            TensorType[torch.float32]: Tensor of shape (batch_size, n_patches + 1, embed_dim), where `n_patches + 1` includes \n                          the class token, and `embed_dim` is the embedding dimension.\n        \"\"\"\n        \n        # patch embedding\n        x = self.unfold(x).transpose(1, 2)\n        x = self.linear_projection(x)\n        \n        # prepending class token\n        batch_size = x.shape[0]\n        \n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        \n        # positional embedding\n        pos_embeddings = self.pos_embedding.expand(batch_size, -1, -1)\n        x = x + pos_embeddings\n\n        return x","metadata":{"_uuid":"3fc513dd-1e01-4827-8e8b-de63d5d9bbcb","_cell_guid":"b31cef00-8677-4392-b88d-e9dc7883c291","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:10.001685Z","iopub.execute_input":"2024-09-30T18:47:10.001985Z","iopub.status.idle":"2024-09-30T18:47:10.014050Z","shell.execute_reply.started":"2024-09-30T18:47:10.001950Z","shell.execute_reply":"2024-09-30T18:47:10.013168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    \"\"\"\n    Vision Transformer (ViT) model for image classification.\n\n    Args:\n        image_size (int, optional): Input image size (square). Default is 224.\n        patch_size (int, optional): Size of image patches. Default is 16.\n        embed_dim (int, optional): Dimensionality of patch embeddings. Default is 192.\n        n_layers (int, optional): Number of transformer encoder layers. Default is 12.\n        n_heads (int, optional): Number of attention heads. Default is 4.\n        mlp_size (int, optional): Size of the MLP layer. Default is 768.\n        n_classes (int, optional): Number of output classes. Default is 10.\n        n_channels (int, optional): Number of input channels (e.g., 3 for RGB). Default is 3.\n        dropout (float, optional): Dropout rate. Default is 0.1.\n        batch_first (bool, optional): If True, the batch dimension comes first. Default is False.\n\n    Attributes:\n        embedding (PatchAndPositionEmbedding): Patch and position embedding layer.\n        transformer_encoder (nn.TransformerEncoder): Transformer encoder with multiple layers.\n        MLP (nn.Sequential): Final classification layer with normalization and linear layers.\n    \"\"\"\n    \n    def __init__(\n        self,\n        image_size: int = 224,\n        patch_size: int = 16,\n        embed_dim: int = 192,\n        n_layers: int = 12,\n        n_heads: int = 4,\n        mlp_size: int = 768,\n        n_classes: int = 10,\n        n_channels: int = 3,\n        dropout: float = 0.1,\n        batch_first: bool = False\n    ) -> None:\n        \"\"\"\n        Initializes the VisionTransformer model.\n\n        Args:\n            image_size (int, optional): Size of the input image. Default is 224.\n            patch_size (int, optional): Size of image patches. Default is 16.\n            embed_dim (int, optional): Dimensionality of patch embeddings. Default is 192.\n            n_layers (int, optional): Number of transformer encoder layers. Default is 12.\n            n_heads (int, optional): Number of attention heads. Default is 4.\n            mlp_size (int, optional): Size of the MLP layer. Default is 768.\n            n_classes (int, optional): Number of output classes. Default is 10.\n            n_channels (int, optional): Number of input channels. Default is 3.\n            dropout (float, optional): Dropout rate. Default is 0.1.\n            batch_first (bool, optional): If True, batch dimension comes first. Default is False.\n        \"\"\"\n        \n        super(VisionTransformer, self).__init__()\n        \n        self.batch_first = batch_first\n        \n        self.embedding = PatchAndPositionEmbedding(image_size, patch_size, embed_dim, n_channels)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=n_heads,\n            dim_feedforward=mlp_size,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=self.batch_first\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        \n        self.MLP = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, n_classes)\n        )\n        \n        self._init_weights()\n                \n    def _init_weights(self):\n        \"\"\"\n        Initializes the weights of the model, including the class token, positional embedding, \n        and the linear projection layer.\n        \"\"\"\n        \n        nn.init.trunc_normal_(self.embedding.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.embedding.pos_embedding, std=0.02)\n        nn.init.xavier_normal_(self.embedding.linear_projection.weight)\n        nn.init.zeros_(self.embedding.linear_projection.bias)\n                    \n    def forward(self, x: TensorType[torch.float32]) -> TensorType[torch.float32]:\n        \"\"\"\n        Forward pass of the VisionTransformer model.\n\n        Args:\n            x (TensorType[torch.float32]): Input image tensor of shape (C, H, W), where C is the number of channels, \n                              and H, W are the height and width of the image.\n\n        Returns:\n            TensorType[torch.float32]: Output logits of shape (batch_size, n_classes), representing the class scores.\n        \"\"\"\n        \n        x = self.embedding(x)\n        \n        if not self.batch_first: \n            x = x.transpose(0, 1)\n        \n        x = self.transformer_encoder(x)\n        \n        cls_token = x[:, 0, :] if self.batch_first else x[0]\n        \n        logits = self.MLP(cls_token)\n        \n        return logits","metadata":{"_uuid":"3b7d6edb-d3fc-4b93-9cd2-d814bc3abed3","_cell_guid":"e6daa07b-4bbe-4e92-8461-f35f47f122e8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:10.015181Z","iopub.execute_input":"2024-09-30T18:47:10.015506Z","iopub.status.idle":"2024-09-30T18:47:10.032062Z","shell.execute_reply.started":"2024-09-30T18:47:10.015463Z","shell.execute_reply":"2024-09-30T18:47:10.030899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer():\n    \"\"\"\n    Trainer class for training and testing a model using distributed data parallel (DDP).\n\n    Args:\n        model (nn.Module): The model to be trained.\n        dataloaders (tuple[DataLoader, DataLoader]): Tuple containing the training and testing dataloaders in this order: (train_dataloader, test_dataloader).\n        optimizer (optim.Optimizer): Optimizer for updating the model parameters.\n        rank (int): Rank of the current process in DDP (GPU ID).\n\n    Attributes:\n        rank (int): GPU rank for DDP.\n        _model (DDP): Distributed Data Parallel wrapped model.\n        _train_dataloader (DataLoader): Dataloader for training data.\n        _test_dataloader (DataLoader): Dataloader for testing data.\n        _optimizer (optim.Optimizer): Optimizer for the model.\n        _loss_fn (nn.CrossEntropyLoss): Loss function for classification tasks.\n        _softmax (nn.Softmax): Softmax function to compute probabilities.\n    \"\"\"\n    \n    def __init__(\n        self,\n        model: nn.Module,\n        dataloaders: tuple[DataLoader, DataLoader],\n        optimizer: optim.Optimizer,\n        rank: int\n    ) -> None:\n        \"\"\"\n        Initializes the Trainer with a model, dataloaders, optimizer, and rank.\n\n        Args:\n            model (nn.Module): The neural network model to be trained.\n            dataloaders (tuple[DataLoader, DataLoader]): Tuple containing the training and testing dataloaders in this order: (train_dataloader, test_dataloader).\n            optimizer (optim.Optimizer): Optimizer used for training the model.\n            rank (int): Rank of the GPU device for DDP.\n        \"\"\"\n        \n        self.rank = rank\n        self._model = nn.DataParallel(model.to(self.rank))\n        self._train_dataloader, self._test_dataloader = dataloaders\n        self._optimizer = optimizer\n        self._loss_fn = nn.CrossEntropyLoss()\n        self._softmax = nn.Softmax(dim=1)\n        \n    def _train_epoch(self) -> None:\n        \"\"\"\n        Performs a single training epoch. Computes loss, backpropagates, and updates model weights.\n        Prints average loss every 10 batches.\n        \"\"\"\n        \n        avg_loss = 0.0\n\n        self._model.train()\n        for batch_index, (X, y) in enumerate(self._train_dataloader, 1):\n            X, y = X.to(self.rank), y.to(self.rank)\n\n            prediction = self._model(X)\n            loss = self._loss_fn(prediction, y)\n\n            avg_loss += loss.item()\n\n            self._optimizer.zero_grad()\n            loss.backward()\n            self._optimizer.step()\n\n            if batch_index % 10 == 0:\n                loss = loss.item()\n                current_sample = batch_index * len(X)\n\n                print(\n                    f'[GPU-{self.rank}]Current loss: {loss:.5f}, ' \\\n                    f'Average loss across 10 batches: {avg_loss / 10:.5f} ' \\\n                    f'[{current_sample} / {len(self._train_dataloader.dataset)}]'\n                )\n                avg_loss = 0.0\n                \n    def _test_epoch(self) -> None:\n        \"\"\"\n        Evaluates the model on the test dataset, computing the average loss and accuracy.\n        \"\"\"\n        \n        self._model.eval()\n        \n        avg_test_loss = 0.0\n        n_correct = 0\n\n        for X, y in self._test_dataloader:\n            X, y = X.to(self.rank), y.to(self.rank)\n\n            with torch.no_grad():\n                prediction = self._model(X)\n\n                loss = self._loss_fn(prediction, y)\n                avg_test_loss += loss.item()\n\n                prediction = self._softmax(prediction)\n                n_correct += (torch.argmax(prediction, dim=1) == y).sum().item()\n\n        print(\n            f'Average test loss: {avg_test_loss / len(self._test_dataloader.dataset):.5f}, ' \\\n            f'Accuracy: {n_correct / len(self._test_dataloader.dataset) * 100:.2f}%'\n        )\n        \n    def train(self, n_epochs: int, path: str | None) -> None:\n        \"\"\"\n        Trains the model for a specified number of epochs and optionally saves the model.\n\n        Args:\n            n_epochs (int): Number of epochs to train the model.\n            path (str | None): If provided, saves the model to the given path after training \n                               (only on rank 0). If `None`, the model is not saved.\n        \"\"\"\n        \n        for epoch in range(n_epochs):\n            print(f\"|--------------------------{epoch + 1}/{N_EPOCHS}--------------------------|\")\n            self._train_epoch()\n            self._test_epoch()\n            \n        if self.rank == 0 and path:\n            self.save_model(path)\n            \n            \n    def save_model(self, path: str) -> None:\n        \"\"\"\n        Saves the model's state dictionary to a specified path.\n\n        Args:\n            path (str): Path where the model will be saved.\n        \"\"\"\n        \n        state = self._model.module.state_dict()\n        torch.save(state, path)\n        print(f'Model saved in: {path}')","metadata":{"_uuid":"54f3e327-0a70-47e0-a15f-00fc427ca7ce","_cell_guid":"0da97fc4-3404-4f83-ab65-18d8d78fe794","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:10.033345Z","iopub.execute_input":"2024-09-30T18:47:10.033688Z","iopub.status.idle":"2024-09-30T18:47:10.051339Z","shell.execute_reply.started":"2024-09-30T18:47:10.033657Z","shell.execute_reply":"2024-09-30T18:47:10.050455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the data","metadata":{"_uuid":"25ed9490-4494-480d-abd0-525d8072af17","_cell_guid":"4c3e250e-1b93-4af4-a9e9-5e42e4f6f95d","trusted":true}},{"cell_type":"code","source":"train_transforms = v2.Compose([\n    v2.Resize((224, 224)),\n    v2.RandomVerticalFlip(p=0.5),\n    v2.PILToTensor(),\n    v2.ToDtype(torch.float32, scale=True)\n])\n\ntest_transforms = v2.Compose([\n    v2.Resize((224, 224)),\n    v2.PILToTensor(),\n    v2.ToDtype(torch.float32, scale=True)\n])","metadata":{"_uuid":"dbb97c7c-fbb0-4220-864d-2566eeac1743","_cell_guid":"16eb8a69-173a-4c90-8f8e-7d26b255049c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:10.052474Z","iopub.execute_input":"2024-09-30T18:47:10.052864Z","iopub.status.idle":"2024-09-30T18:47:10.065389Z","shell.execute_reply.started":"2024-09-30T18:47:10.052823Z","shell.execute_reply":"2024-09-30T18:47:10.064459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ImageFolder(\n    root='/kaggle/input/packed-fruits-and-vegetables-recognition-benchmark/train/train',\n    transform=train_transforms\n)\ntest_dataset = ImageFolder(\n    root='/kaggle/input/packed-fruits-and-vegetables-recognition-benchmark/test/test',\n    transform=test_transforms\n)","metadata":{"_uuid":"226d0461-d8ed-4d7a-b03b-67f4b17c94ad","_cell_guid":"47623a93-0955-4bb4-86ca-9bf2648bf283","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:10.069189Z","iopub.execute_input":"2024-09-30T18:47:10.069487Z","iopub.status.idle":"2024-09-30T18:47:42.459657Z","shell.execute_reply.started":"2024-09-30T18:47:10.069457Z","shell.execute_reply":"2024-09-30T18:47:42.458853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataloaders() -> tuple[DataLoader, DataLoader]:\n    return (\n        DataLoader(\n            train_dataset,\n            batch_size=256,\n            shuffle=True,\n            pin_memory=True,\n            num_workers=4,\n#             sampler=DistributedSampler(train_dataset)\n        ),\n        DataLoader(\n            test_dataset,\n            batch_size=256,\n            shuffle=False,\n            pin_memory=True,\n            num_workers=4,\n#             sampler=DistributedSampler(test_dataset)\n        )\n    )","metadata":{"_uuid":"3687cbcf-7b2e-4445-ad34-e3f78a1f606a","_cell_guid":"087bd2ae-566e-4107-8f32-7cea3ff1f7c0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:42.460731Z","iopub.execute_input":"2024-09-30T18:47:42.461033Z","iopub.status.idle":"2024-09-30T18:47:42.467399Z","shell.execute_reply.started":"2024-09-30T18:47:42.461002Z","shell.execute_reply":"2024-09-30T18:47:42.466317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = test_dataset[0]","metadata":{"_uuid":"4600e623-1bfa-43a5-8262-28fcbd71e78d","_cell_guid":"ceb99711-d668-4a1e-96e4-14dc7d97be5c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:42.468441Z","iopub.execute_input":"2024-09-30T18:47:42.468723Z","iopub.status.idle":"2024-09-30T18:47:42.587012Z","shell.execute_reply.started":"2024-09-30T18:47:42.468693Z","shell.execute_reply":"2024-09-30T18:47:42.586003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx_to_class = train_dataset.find_classes(train_dataset.root)[0]\nidx_to_class","metadata":{"_uuid":"58d2a62c-1de6-4778-9fe4-45a79af12855","_cell_guid":"89e50a17-015f-427c-98c9-b3bfc9dc271e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:42.588206Z","iopub.execute_input":"2024-09-30T18:47:42.588561Z","iopub.status.idle":"2024-09-30T18:47:42.600457Z","shell.execute_reply.started":"2024-09-30T18:47:42.588524Z","shell.execute_reply":"2024-09-30T18:47:42.599379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title(idx_to_class[y])\nplt.imshow(X.permute(1, 2, 0))\nplt.show()","metadata":{"_uuid":"85db156d-1c8c-4d7d-8a98-fa730b3e1985","_cell_guid":"e0baf018-1b85-4992-bcbb-7f0975999a3d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:42.601714Z","iopub.execute_input":"2024-09-30T18:47:42.602006Z","iopub.status.idle":"2024-09-30T18:47:42.983056Z","shell.execute_reply.started":"2024-09-30T18:47:42.601975Z","shell.execute_reply":"2024-09-30T18:47:42.982148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"_uuid":"f35b723f-13fb-4381-8833-2957699c4778","_cell_guid":"cfabeead-1d45-4617-8089-d460948f429d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:42.984288Z","iopub.execute_input":"2024-09-30T18:47:42.984655Z","iopub.status.idle":"2024-09-30T18:47:42.991086Z","shell.execute_reply.started":"2024-09-30T18:47:42.984616Z","shell.execute_reply":"2024-09-30T18:47:42.990168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the ViT","metadata":{"_uuid":"0de2e9bb-b004-4541-a704-3d1caaf3d9d7","_cell_guid":"0e38558d-7c2d-4c7d-82cb-6f7851c60eed","trusted":true}},{"cell_type":"code","source":"ViT = VisionTransformer(batch_first=True, n_classes=len(idx_to_class))","metadata":{"_uuid":"6f21412a-537e-47e6-bed3-03806a15c078","_cell_guid":"65ab7497-95db-4299-bc13-6a4089a8a11c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:42.992094Z","iopub.execute_input":"2024-09-30T18:47:42.992398Z","iopub.status.idle":"2024-09-30T18:47:43.091129Z","shell.execute_reply.started":"2024-09-30T18:47:42.992366Z","shell.execute_reply":"2024-09-30T18:47:43.090312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 10\nLEARNING_RATE = 8e-5\n\noptimizer = optim.AdamW(ViT.parameters(), lr=LEARNING_RATE)","metadata":{"_uuid":"b0dbdf84-30f6-4089-a2b3-55e80e61e761","_cell_guid":"53be63ef-9faf-4db7-9b55-3ecd201cbdb0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-30T18:47:43.092236Z","iopub.execute_input":"2024-09-30T18:47:43.092613Z","iopub.status.idle":"2024-09-30T18:47:43.097996Z","shell.execute_reply.started":"2024-09-30T18:47:43.092564Z","shell.execute_reply":"2024-09-30T18:47:43.097066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rank = 'cuda'\ntrainer = Trainer(ViT, get_dataloaders(), optimizer, rank)\ntrainer.train(n_epochs=N_EPOCHS, path=f'ViT-1-{date.today().isoformat()}.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T18:47:43.099112Z","iopub.execute_input":"2024-09-30T18:47:43.099476Z","iopub.status.idle":"2024-09-30T20:53:27.935657Z","shell.execute_reply.started":"2024-09-30T18:47:43.099435Z","shell.execute_reply":"2024-09-30T20:53:27.934444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TODO:\n- **DONE** revert this change as multiprocessing does not work in jupyter **DONE** replace `nn.DataParallel` with `nn.parallel.DistribiutedDataParallel`\n- **DONE** use `X, y = test_dataset[0]` instead of `X, y = next(iter(test_dataloader))` to free some RAM\n- **DONE** maybe change the learning rate to `8e-5` to see if model converges faster\n- add more markdown cells explaining each step better","metadata":{"_uuid":"7cd1225e-6b8b-4018-8d47-71de99219a2f","_cell_guid":"1b1cd88c-dcba-4177-8b40-52093be61ff2","trusted":true}},{"cell_type":"markdown","source":"# Models' parameters:\n - ViT-1: \n     - image_size: 224,\n     - patch_size: 16,\n     - embed_dim: 192,\n     - n_layers: 12,\n     - n_heads: 4,\n     - mlp_size: 768,\n     - n_classes: 65,\n     - n_channels: 3,\n     - dropout: 0.1,\n     - batch_first: True","metadata":{"_uuid":"4f94a64d-26c6-4722-97a1-80884b4bc5b7","_cell_guid":"f90d9d8e-dd41-438a-bc71-7be0236290b7","trusted":true}}]}